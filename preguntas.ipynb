{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDT2 \n",
    "\n",
    "## Task 1 - Preguntas Teóricas\n",
    "\n",
    "1. **Defina el proceso de decisión de Markov (MDP) y explique sus componentes.**\n",
    "MDP es un modelo matemático que se usa para tomar decisiones que pueden tener resultados parcialmente aleatorios y dependen del estado en el que esté el sistema. Sus componentes son:<br>\n",
    "Estado -> Conjunto de estados del agente. <br>\n",
    "s_start -> Estado inicial. <br>\n",
    "Acciones -> Posibles acciones que puede tomar el agente. <br>\n",
    "T(s,a,s') -> Probabilidad de s' si la acción a se toma en el estado s. <br>\n",
    "Reward(s,a,s') -> Recompensa de la transición. <br>\n",
    "IsEnd(s) -> Determina si ya terminó el juego.\n",
    "gamma (factor de descuento) -> Valor entre 0 y 1 que determina el valor de las recompensas futuras en comparación a las inmediatas. <br>\n",
    "\n",
    "2. **Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.**\n",
    "La *política* es la estrategia con la que el agente toma decisiones. Tiene dos formas, determinista y estocástica. La determinista siempre tiene una acción fija por estado, mientras que la estocástica le asigna probabilidades a diferentes acciones en el mismo estado. La *evaluación de políticas* es el cálculo del valor esperado para seguir una política. Se hace mediante la función de valor que mide la recompensa esperada si se sigue una cierta política. Luego de evaluarlas podemos ver cuales son las políticos con peor rendimiento. Aquí es donde entra la *mejora de políticas*, que es cuando se cambia la elección de acciones para maximizar la recompensa. Este se hace usando la función de acción-valor que mide la recompensa según un estado y una acción. Por último, la *iterarción de políticas* es cuando se combinan la evaluación y optimización y así encontrar la mejor política para el caso. <br>\n",
    "\n",
    "3. **Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?**\n",
    "Es el parámetro que muestra la eficiencia deseada para alcanzar la recompensa. Este lo hace con un valor numérico entre el 0 y el 1. Mientras más cercano a 1 sea el valor, más larga será la transición, mientras que si el valor se acerca más a 0 entonces la transición será más carta. <br>\n",
    "\n",
    "4. **Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.**\n",
    "La iteración de valores se basa en actualizar repetidamente la función de valor hasta llegar a una convergencia. Después, se extrae la polémica óptima. La iteración de políticas combina la evaluación y la mejora de estas. En pocas palabras, el primero calcula primero el valor de los estados y después toma una desición, mientras que el segundo va probando estrategia por estrategia. Cada una tiene sus ventajas y desventajas que hacen que sea mejor usarla en algunos escenarios más que en otros. El de valores es más rápido por lo que es ideal para casos grandes, mientras que el otro es más exacto y es mejor usarlo en problemas más pequeños ya que puede llegar a ser pesado.<br>\n",
    "\n",
    "5. **¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.**\n",
    "El problema es que el proceso de resolución más exacto (el de iteración de políticas) es más costoso por lo que es más lento. Esto debido a que los resultados de la función de valor y de acción valor se deben guardar y actualizar con cada nueva entrada que reciba el agente. Otro problema puede ser que para resolver los MDP se deben conocer algunos factores como la función de transición, sin embargo en la práctica es raro conocer ese valor. Para abordar estos desafios se pueden utilizar diferentees algotimos y métodos con los que se calculan los valores y transiciones. Por ejemplo, los diferentes algoritmos de aproximación sirven para evitar guardar el valor y aproximandolo en su lugar. Otra forma de abordarlo está en buscar optimizar las transiciones, lo que se puede hacer con el método de aprendizaje por refuerzo y así no tener la necesidad de conocer las transiciones, lo que también vuelve más rápido abordarlas.\n",
    "\n",
    "## Task 2 - Preguntas Analíticas\n",
    "\n",
    "1. **Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones para la toma de decisiones.**\n",
    "La propiedad de Markov establece que el futuro depende únicamente del futuro y no del pasado y muestra como un estado actual puede contener toda la información necesaria para tomar decisiones. El problema es que en la vida real pueden llegar a existir casos donde los supuestos subyacentes a la propiedad no se cumplan y por lo tanto esta falle. Los supuestos y los casos donde pueden fallar son:<br>\n",
    "- Memoria corta y sin conexión al pasado: existen muchos casos donde se requiere de un transfondo más profundo para saber como actuar de la manera más eficiente. Por ejemplo un doctor necesita saber el historial médico completo de un paciente y no solo fiarse de su diagnóstico actual. Otro puede ser el caso de un juez que debe conocer todo el contexto del caso antes de tomar un veredícto y no solo confiar en los hechos más recientes.\n",
    "- Las reglas del proceso de decisión no cambian con el tiempo: en la vida real, muchas cosas van cambiando con el tiempo, volviendo obsoletas algunas reglas y creando nuevas. Esto puede verse, por ejemplo, con un juez que no puede crear un veredicto basado en reglas o leyes de hace 100 años. Otro ejemplo puede ser intentar calcular los precios de algo sin tomar en cuenta como va variando la economía con el paso del tiempo.\n",
    "- Se puede conocer todo el entorno en todo momento: esto también puede presentar problemas en la vida real, ya que es imposible conocer todas las variables en todas las situaciones. Por ejemplo, manejando es imposible saber que harán los otros conductores.<br>\n",
    "Por lo tanto, aunque los MDP pueden ser útiles en ciertas ocasiones, para casos más complejos puede llegar a tomar informaciones \"precipitadas\" o incorrectas.\n",
    "\n",
    "2. **Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice estrategias para una toma de decisiones sólida en entornos inciertos.**\n",
    "Existen diferentes incertidumbres que se pueden dar en un MDP que pueden afectar a la toma de decisiones. Estos son:\n",
    "- Incertidumbre en la dinámica: no siempre se conoce la probabilidad exacta para una transición.\n",
    "- Incertidumbre en las recompensas: estas pueden llegar a cambiar con el tiempo o simplemente ser desconocidas desde un principio.\n",
    "- Incertidumbre en la observaación del estado: a veces no se puede observar todo el estado o entorno de la situación por lo que se dificulta crear una decisión certera.<br>\n",
    "Para tomar mejores decisiones en entornos donde pueden estar presentes estas incertidumbres podemos recurrir a varias herramientas. La primera puede ser el aprendizaje por refuerzo, para el cuál se debe entrenar al agente con casos similares para generar experiencia. Otro podría ser el uso de políticas estocásticas, es decir no definir una misma acción siempre para un estado sino darle una probabilidad a varias acciones por estado. Esto ofrece más opciones, lo que permite una mejor toma de decisiones. Otra puede ser tomar decisiones según la misma incertidumbre y darle más validez a la decisión con menos riesgo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Preguntas Practicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagoMaker(semilla=42):\n",
    "    random.seed(semilla)\n",
    "    np.random.seed(semilla)\n",
    "    \n",
    "    tam = 4  # Tablero de 4x4\n",
    "    lago = np.full((tam, tam), \"F\")  # 'F' de piso Frozen\n",
    "    \n",
    "    # Posiciones iniciales\n",
    "    esquinas = [(0, 0), (0, 3), (3, 0), (3, 3)]\n",
    "    inicio = random.choice(esquinas)\n",
    "    meta = (tam - 1 - inicio[0], tam - 1 - inicio[1])  # Meta en la esquina opuesta\n",
    "    \n",
    "    lago[inicio] = 'S'\n",
    "    lago[meta] = 'G'\n",
    "    \n",
    "    # Elegir cuántos hoyos poner (entre 1 y 3)\n",
    "    num_hoyos = random.randint(1, 3)\n",
    "    hoyos = set()\n",
    "    \n",
    "    # Colocar hoyos sin pisar inicio o meta\n",
    "    while len(hoyos) < num_hoyos:\n",
    "        h = (random.randint(0, tam - 1), random.randint(0, tam - 1))\n",
    "        if h != inicio and h != meta:\n",
    "            hoyos.add(h)\n",
    "    \n",
    "    # Marcar los hoyos en el lago\n",
    "    for h in hoyos:\n",
    "        lago[h] = 'H'\n",
    "    \n",
    "    return lago, inicio, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ver_lago(lago):\n",
    "    for fila in lago:\n",
    "        print(\" \".join(fila))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mov_pliticas(lago, inicio, meta):\n",
    "    tam = lago.shape[0]\n",
    "    \n",
    "    # Movimientos posibles\n",
    "    movimientos = {\n",
    "        '↑': (-1, 0), \n",
    "        '↓': (1, 0), \n",
    "        '←': (0, -1), \n",
    "        '→': (0, 1)\n",
    "    }\n",
    "    \n",
    "    # Crear un tablero vacío para la política\n",
    "    politica = np.full((tam, tam), 'X', dtype=str)  # 'X' es que no paso por el\n",
    "    \n",
    "    for i in range(tam):\n",
    "        for j in range(tam):\n",
    "            if lago[i, j] in ('H', 'G'):  # Si es un hoyo o la meta, no hay que moverse\n",
    "                politica[i, j] = lago[i, j]\n",
    "            else:\n",
    "                mejor_mov = None\n",
    "                mejor_dist = float('inf')\n",
    "                \n",
    "                for mov, (di, dj) in movimientos.items():\n",
    "                    ni, nj = i + di, j + dj\n",
    "                    \n",
    "                    if 0 <= ni < tam and 0 <= nj < tam and lago[ni, nj] != 'H':\n",
    "                        # Calcular distancia a la meta (distancia Manhattan)\n",
    "                        dist = abs(meta[0] - ni) + abs(meta[1] - nj)\n",
    "                        \n",
    "                        if dist < mejor_dist:\n",
    "                            mejor_dist = dist\n",
    "                            mejor_mov = mov\n",
    "                \n",
    "                politica[i, j] = mejor_mov if mejor_mov else 'X'  # Si no hay opción, deja 'X'\n",
    "    \n",
    "    return politica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ver_politica(politica):\n",
    "    for fila in politica:\n",
    "        print(\" \".join(fila))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapa del lago congelado:\n",
      "S F F F\n",
      "F F F F\n",
      "F H F F\n",
      "F F F G\n",
      "\n",
      "Política del agente:\n",
      "↓ ↓ ↓ ↓\n",
      "↓ → ↓ ↓\n",
      "↓ H ↓ ↓\n",
      "→ → → G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hacer el lago congelado\n",
    "lago, inicio, meta = lagoMaker()\n",
    "print(\"Mapa del lago congelado:\")\n",
    "ver_lago(lago)\n",
    "\n",
    "# Generar la política del agente\n",
    "politica = mov_pliticas(lago, inicio, meta)\n",
    "print(\"Política del agente:\")\n",
    "ver_politica(politica)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
