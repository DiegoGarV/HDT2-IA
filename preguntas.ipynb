{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDT2 \n",
    "\n",
    "## Task 1 - Preguntas Teóricas\n",
    "\n",
    "1. **Defina el proceso de decisión de Markov (MDP) y explique sus componentes.**\n",
    "MDP es un modelo matemático que se usa para tomar decisiones que pueden tener resultados parcialmente aleatorios y dependen del estado en el que esté el sistema. Sus componentes son:<br>\n",
    "Estado -> Conjunto de estados del agente. <br>\n",
    "s_start -> Estado inicial. <br>\n",
    "Acciones -> Posibles acciones que puede tomar el agente. <br>\n",
    "T(s,a,s') -> Probabilidad de s' si la acción a se toma en el estado s. <br>\n",
    "Reward(s,a,s') -> Recompensa de la transición. <br>\n",
    "IsEnd(s) -> Determina si ya terminó el juego.\n",
    "gamma (factor de descuento) -> Valor entre 0 y 1 que determina el valor de las recompensas futuras en comparación a las inmediatas. <br>\n",
    "\n",
    "2. **Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.**\n",
    "La *política* es la estrategia con la que el agente toma decisiones. Tiene dos formas, determinista y estocástica. La determinista siempre tiene una acción fija por estado, mientras que la estocástica le asigna probabilidades a diferentes acciones en el mismo estado. La *evaluación de políticas* es el cálculo del valor esperado para seguir una política. Se hace mediante la función de valor que mide la recompensa esperada si se sigue una cierta política. Luego de evaluarlas podemos ver cuales son las políticos con peor rendimiento. Aquí es donde entra la *mejora de políticas*, que es cuando se cambia la elección de acciones para maximizar la recompensa. Este se hace usando la función de acción-valor que mide la recompensa según un estado y una acción. Por último, la *iterarción de políticas* es cuando se combinan la evaluación y optimización y así encontrar la mejor política para el caso. <br>\n",
    "\n",
    "3. **Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?**\n",
    "Es el parámetro que muestra la eficiencia deseada para alcanzar la recompensa. Este lo hace con un valor numérico entre el 0 y el 1. Mientras más cercano a 1 sea el valor, más larga será la transición, mientras que si el valor se acerca más a 0 entonces la transición será más carta. <br>\n",
    "\n",
    "4. **Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.**\n",
    "La iteración de valores se basa en actualizar repetidamente la función de valor hasta llegar a una convergencia. Después, se extrae la polémica óptima. La iteración de políticas combina la evaluación y la mejora de estas. En pocas palabras, el primero calcula primero el valor de los estados y después toma una desición, mientras que el segundo va probando estrategia por estrategia. Cada una tiene sus ventajas y desventajas que hacen que sea mejor usarla en algunos escenarios más que en otros. El de valores es más rápido por lo que es ideal para casos grandes, mientras que el otro es más exacto y es mejor usarlo en problemas más pequeños ya que puede llegar a ser pesado.<br>\n",
    "\n",
    "5. **¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.**\n",
    "El problema es que el proceso de resolución más exacto (el de iteración de políticas) es más costoso por lo que es más lento. Esto debido a que los resultados de la función de valor y de acción valor se deben guardar y actualizar con cada nueva entrada que reciba el agente. Otro problema puede ser que para resolver los MDP se deben conocer algunos factores como la función de transición, sin embargo en la práctica es raro conocer ese valor. Para abordar estos desafios se pueden utilizar diferentees algotimos y métodos con los que se calculan los valores y transiciones. Por ejemplo, los diferentes algoritmos de aproximación sirven para evitar guardar el valor y aproximandolo en su lugar. Otra forma de abordarlo está en buscar optimizar las transiciones, lo que se puede hacer con el método de aprendizaje por refuerzo y así no tener la necesidad de conocer las transiciones, lo que también vuelve más rápido abordarlas.\n",
    "\n",
    "## Task 2 - Preguntas Analíticas\n",
    "\n",
    "1. **Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones para la toma de decisiones.**\n",
    "\n",
    "\n",
    "2. **Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice estrategias para una toma de decisiones sólida en entornos inciertos.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
